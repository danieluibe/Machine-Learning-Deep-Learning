{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Project 8: Deep Learning</h2>\n",
    "\n",
    "\n",
    "<h3>Introduction</h3>\n",
    "In this project, you will implement a neural network.\n",
    "We broke it apart into several functions and a pre-processing step.</p>\n",
    "\n",
    "  <p> \n",
    "  An important step to make neural networks function properly is to scale the data properly. Therefore, first implement the preprocess function\n",
    "  It takes as input the training and the test data and transforms them both with the same mapping: $\\vec x_i\\rightarrow U\\vec x_i-\\vec{m}$. \n",
    "  After this transformation the training data set should have zero-mean and each feature should have standard deviation 1 . The same transformation is then also applied to the test data (Hint: The matrix U is typically diagonal.) \n",
    "  <!--\t <li>HINT 2: Ideally you would like the input features to be de-correlated. The correlation matrix should be diagonal (in this case even the identity matrix). One way to do this is to project the data onto the PCA principal components (which we will still cover later in the course). You can get the transposed projection matrix by calling $pcacov(xTr')$. Make sure to apply PCA <i>after</i> you subtracted off the mean. </li>-->\n",
    "  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition preprocess(Any, Any) in module Main at In[3]:16 overwritten at In[4]:16.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "preprocess (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MAT\n",
    "#<GRADED>\n",
    "function preprocess(xTr,xTe)\n",
    "# function [xTr,xTe,u,m]=preprocess(xTr,xTe);\n",
    "#\n",
    "# Preproces the data to make the training features have zero-mean and\n",
    "# standard-deviation 1\n",
    "#\n",
    "# output:\n",
    "# xTr - pre-processed training data \n",
    "# xTe - pre-processed testing data\n",
    "# \n",
    "# u,m - any other data should be pre-processed by x-> u*(x-m)\n",
    "#\n",
    "\n",
    "    Ntr,D=size(xTr);\n",
    "    Nte,D=size(xTe);\n",
    "\n",
    "    ##  Todo 1\n",
    "    \n",
    "    return (xTr,xTe,u,m)\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<p>We need to define some transition functions and their gradients. Implement the sigmoid, tanh, and ReLU transition functions (the $sin()$ transition function is already implemented for you).</p> \n",
    " <ol>\n",
    "     <li>ReLU: $\\sigma(z) = \\max(0, z)$</li>\n",
    "     <li>Sigmoid: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$</li>\n",
    "     <li>tanh: $\\sigma(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$</li>\n",
    " </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_transition_func (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#<GRADED>\n",
    "function get_transition_func(transtype)\n",
    "#\tGiven the type, gets a specific transition function\n",
    "#   INPUT\n",
    "#   type \"sigmoid\", \"tanh\", \"ReLU\"\n",
    "#   OUTPUT\n",
    "#   trans_func transition function (function)\n",
    "#   trans_func_der derivative of the transition function (function)\n",
    "\n",
    "    # type must be one of the defined transition functions            \n",
    "    assert(transtype in [\"sigmoid\", \"tanh\", \"ReLU\",\"sin\"])\n",
    "            \n",
    "    if transtype==\"sin\"      \n",
    "            trans_func=z->sin(z);  \n",
    "            trans_func_der=z->cos(z);                                      \n",
    "    end\n",
    "\n",
    "    ## Todo 2\n",
    "    \n",
    "    return (trans_func,trans_func_der)\n",
    "end\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If everything is correct we can now plot these functions and check their gradients. The gradient errors should all be very small (less than $10^{-10}$.) </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1,#2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f, delta_f = get_transition_func(\"sin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: trans_func not defined\nwhile loading In[6], in expression starting on line 17",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: trans_func not defined\nwhile loading In[6], in expression starting on line 17",
      "",
      " in get_transition_func(::String) at ./In[5]:20",
      " in macro expansion; at ./In[6]:20 [inlined]",
      " in anonymous at ./<missing>:?"
     ]
    }
   ],
   "source": [
    "using PyPlot\n",
    "\n",
    "function numericalgradient(fun,x,e);\n",
    "    dh=0;\n",
    "    nx=x;    # copy the weight vector\n",
    "    nx+=e;  # perturn dimension i\n",
    "    l1=fun(nx); # compute loss\n",
    "    nx-=2*e; # perturm dimension i again\n",
    "    l2=fun(nx); # compute loss\n",
    "    dh=(l1[1]-l2[1])/(2*e); # the gradient is the slope of the loss\n",
    "    return(dh);\n",
    "end;\n",
    "\n",
    "\n",
    "figure()\n",
    "names=[\"sigmoid\",\"tanh\",\"ReLU\",\"sin\"]\n",
    "for i=1:4\n",
    "    name=names[i];\n",
    "    subplot(1,4,i)\n",
    "    f,δf=get_transition_func(name)\n",
    "    x=-5:0.1:5;\n",
    "    plot(x,f(x))\n",
    "    title(name)\n",
    "    grid(true);\n",
    "    axis([-5,5,-1,1])\n",
    "    \n",
    "    \n",
    "    print(\"$(name) gradient check at x=1\")\n",
    "    dh=numericalgradient(f,1,1e-5);\n",
    "    dy=δf(1);\n",
    "    graderror=norm(dh-dy)/norm(dh+dy)\n",
    "    if graderror<1e-10; print(\"✔\");else; print(\"✘\")end;\n",
    "\n",
    "    print(\" at x=-1\")\n",
    "    dh2=numericalgradient(f,-1,1e-5);\n",
    "    dy2=δf(-1);\n",
    "    graderror+=norm(dh2-dy2)/norm(dh2+dy2)\n",
    "    if graderror<1e-10; print(\"✔\");else; print(\"✘\")end;\n",
    "    print(\"\\n\");\n",
    " end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  <p> Now implement the forward pass function\n",
    "  <pre>forward_pass(W,xTr,trans_func)</pre>\n",
    "  It takes the weights for the network, the training data, and the transition function to be used between layers.  It should output the result at each node for the forward pass.  $W\\{1\\}$ stores the weights for the last layer of the network and $W\\{end\\}$ the <i>first</i> layer.\n",
    "  </p>\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function initweights(specs)\n",
    "    weights=[]\n",
    "    for i=1:length(specs)-1\n",
    "        append!(weights,[randn(specs[i],specs[i+1])])        \n",
    "    end;    \n",
    "    return(weights)\n",
    "end;    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward_pass (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#<GRADED>\n",
    "function forward_pass(weights, xTr, trans_func )\n",
    "# function forward_pass(weights,xTr,trans_func)\n",
    "# \n",
    "# INPUT:\n",
    "# weights weights (cell array)\n",
    "# xTr nxd matrix (each column is an input vector)\n",
    "# trans_func transition function to apply for inner layers\n",
    "#\n",
    "# OUTPUTS:\n",
    "# as = result of forward pass \n",
    "# zs = result of forward pass (zs[end] output layer of the forward pass) \n",
    "#\n",
    "\n",
    "    # Todo 3\n",
    "    return(as,zs)\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: initweights not defined\nwhile loading In[7], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: initweights not defined\nwhile loading In[7], in expression starting on line 1",
      ""
     ]
    }
   ],
   "source": [
    "weights=initweights([2,20,20,1])\n",
    "f,δf=get_transition_func(\"sigmoid\")\n",
    "x=rand(100,2);\n",
    "as,zs=forward_pass(weights,x,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <p> Now compute the loss for the network\n",
    "  <pre>compute_loss(zs, yTr)</pre>\n",
    "  It takes the output of the forward pass and the training labels.  It should compute the loss for the entire training set averaging over all the points:\n",
    "  $$L(x, y) = \\frac{1}{2n}(H(x) - y)^2$$\n",
    "  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition compute_loss(Any, Any) in module Main at In[1]:14 overwritten at In[8]:14.\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: zs not defined\nwhile loading In[8], in expression starting on line 25",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: zs not defined\nwhile loading In[8], in expression starting on line 25",
      ""
     ]
    }
   ],
   "source": [
    "#<GRADED>\n",
    "function compute_loss(zs, yTr)\n",
    "# function [loss] = compute_loss(zs, yTr)\n",
    "#\n",
    "# INPUT:\n",
    "# zs output of forward pass (cell array)\n",
    "# yTr 1xn matrix (each entry is a label)\n",
    "#\n",
    "# OUTPUTS:\n",
    "# \n",
    "# loss = the average squared loss obtained with zs and yTr\n",
    "#\n",
    "#\n",
    "    delta=zs[end][:]-yTr[:];\n",
    "    n=length(yTr);\n",
    "    loss=0;\n",
    "    \n",
    "    ## Todo 4\n",
    "    \n",
    "    return(loss)\n",
    "end;\n",
    "#</GRADED>\n",
    "\n",
    "yTr=rand(100);\n",
    "compute_loss(zs,yTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p> Now implement <i>back propagation</i> \n",
    "  <pre>backprop(W, as, zs, yTr,  der_trans_func)</pre>\n",
    "  to compute the gradient for the weights and bias terms. \n",
    "  It takes the weights for the network, the outputs of the forward pass, the training labels, and the derivative of the transition function.  Use the chain rule to calculate the gradient of the weights.\n",
    "  </p>\n",
    "  \t\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#<GRADED>\n",
    "function  backprop(weights, as,zs, yTr,  δf)\n",
    "# function [gradient] = backprop(W, as, zs, yTr,  der_trans_func)\n",
    "#\n",
    "# INPUT:\n",
    "# W weights (cell array)\n",
    "# as output of forward pass (cell array)\n",
    "# zs output of forward pass (cell array)\n",
    "# yTr 1xn matrix (each entry is a label)\n",
    "# der_trans_func derivative of transition function to apply for inner layers\n",
    "#\n",
    "# OUTPUTS:\n",
    "# \n",
    "# gradient = the gradient at w as a cell array of matrices\n",
    "#\n",
    "    n=length(yTr);\n",
    "    δ=zs[end]-yTr;\n",
    "    # compute gradient with back-prop\n",
    "    gradients=copy(weights);    \n",
    "    for i=length(weights):-1:1\n",
    "        # Todo 5\n",
    "    end; \n",
    "    return(gradients)\n",
    " end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: initweights not defined\nwhile loading In[10], in expression starting on line 7",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: initweights not defined\nwhile loading In[10], in expression starting on line 7",
      ""
     ]
    }
   ],
   "source": [
    "x=(0:0.1:5)[:]\n",
    "y=(x.^2.+sin(x).*10)[:];\n",
    "#y=3.*x;\n",
    "\n",
    "x2d=[x x.*0.+1];\n",
    "\n",
    "weights=initweights([2,200,1])\n",
    "#weights[1]=3;\n",
    "Momentum=copy(weights);\n",
    "for j=1:length(weights)\n",
    "   Momentum[j]*=0.0;\n",
    "end;    \n",
    "\n",
    "\n",
    "α=0.01;\n",
    "M=10000;\n",
    "β=0.8;\n",
    "losses=zeros(M)\n",
    "tic()\n",
    "for i=1:M\n",
    "    f,δf=get_transition_func(\"ReLU\")\n",
    "    as,zs=forward_pass(weights,x2d,f)\n",
    "    losses[i]=compute_loss(zs,y)\n",
    "    gradients=backprop(weights,as,zs,y,δf)\n",
    "    for j=1:length(weights)\n",
    "        Momentum[j]=Momentum[j].*β+(1-β).*gradients[j];\n",
    "        weights[j]-=α.*Momentum[j];\n",
    "    end;    \n",
    "end;\n",
    "print(toc())\n",
    "subplot(1,2,1);\n",
    "plot(x,y,\"b\");\n",
    "plot(x,zs[end],\"r\")\n",
    "subplot(1,2,2);\n",
    "semilogy(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=(0:0.1:5)[:]\n",
    "y=(x.^2.+sin(x).*10)[:];\n",
    "#y=3.*x;\n",
    "\n",
    "x2d=[x x.*0.+1];\n",
    "\n",
    "weights=initweights([2,200,1])\n",
    "\n",
    "\n",
    "hist_grad=copy(weights);\n",
    "for j=1:length(weights)\n",
    "    hist_grad[j]*=0.0;\n",
    "end; \n",
    "\n",
    "\n",
    "α=0.01;\n",
    "M=10000;\n",
    "β=0.8;\n",
    "eps = 1e-6\n",
    "\n",
    "losses=zeros(M)\n",
    "tic()\n",
    "for i=1:M\n",
    "    f,δf=get_transition_func(\"ReLU\")\n",
    "    as,zs=forward_pass(weights,x2d,f)\n",
    "    losses[i]=compute_loss(zs,y)\n",
    "    gradients=backprop(weights,as,zs,y,δf)\n",
    "    for j=1:length(weights)\n",
    "        hist_grad[j] += gradients[j].^2\n",
    "        adj_grad = gradients[j] ./ (eps + hist_grad[j].^0.5)\n",
    "        weights[j]-=α.*adj_grad;\n",
    "    end;    \n",
    "end;\n",
    "print(toc())\n",
    "subplot(1,2,1);\n",
    "plot(x,y,\"b\");\n",
    "plot(x,zs[end],\"r\")\n",
    "subplot(1,2,2);\n",
    "semilogy(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom step-size decrease schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=(0:0.1:5)[:]\n",
    "y=(x.^2.+sin(x).*10)[:];\n",
    "#y=3.*x;\n",
    "\n",
    "x2d=[x x.*0.+1];\n",
    "\n",
    "weights=initweights([2,200,1])\n",
    "\n",
    "\n",
    "hist_grad=copy(weights);\n",
    "for j=1:length(weights)\n",
    "    hist_grad[j]*=0.0;\n",
    "end; \n",
    "\n",
    "\n",
    "α=0.01;\n",
    "M=10000;\n",
    "β=0.8;\n",
    "eps = 1e-6\n",
    "\n",
    "losses=zeros(M)\n",
    "tic()\n",
    "for i=1:M\n",
    "    f,δf=get_transition_func(\"ReLU\")\n",
    "    as,zs=forward_pass(weights,x2d,f)\n",
    "    losses[i]=compute_loss(zs,y)\n",
    "    gradients=backprop(weights,as,zs,y,δf)\n",
    "    for j=1:length(weights)\n",
    "        if i != 0\n",
    "            hist_grad[j] = β * hist_grad[j] + (1 - β) * gradients[j].^2\n",
    "        else\n",
    "            hist_grad[j] = gradients[j].^2\n",
    "        end\n",
    "        adj_grad = gradients[j] ./ (eps + hist_grad[j].^0.5)\n",
    "        weights[j]-=α.*adj_grad;\n",
    "    end;    \n",
    "end;\n",
    "print(toc())\n",
    "subplot(1,2,1);\n",
    "plot(x,y,\"b\");\n",
    "plot(x,zs[end],\"r\")\n",
    "subplot(1,2,2);\n",
    "semilogy(losses)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Project 8: Deep Learning</h2>\n",
    "\n",
    "\n",
    "<h3>Introduction</h3>\n",
    "In this project, you will implement a neural network.\n",
    "We broke it apart into several functions and a pre-processing step.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "#</GRADED>\n",
    "import matplotlib\n",
    "matplotlib.use('PDF')\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "# add p08 folder\n",
    "sys.path.insert(0, './p08/')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <p> \n",
    "  An important step to make neural networks function properly is to scale the data properly. Therefore, first implement the preprocess function\n",
    "  It takes as input the training and the test data and transforms them both with the same mapping: $\\vec x_i\\rightarrow U\\vec x_i-\\vec{m}$. \n",
    "  After this transformation the training data set should have zero-mean and each feature should have standard deviation 1 . The same transformation is then also applied to the test data (Hint: The matrix U is typically diagonal.) \n",
    "  <!--\t <li>HINT 2: Ideally you would like the input features to be de-correlated. The correlation matrix should be diagonal (in this case even the identity matrix). One way to do this is to project the data onto the PCA principal components (which we will still cover later in the course). You can get the transposed projection matrix by calling $pcacov(xTr')$. Make sure to apply PCA <i>after</i> you subtracted off the mean. </li>-->\n",
    "  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def preprocess(xTr, xTe):\n",
    "    \"\"\"\n",
    "    Preproces the data to make the training features have zero-mean and\n",
    "    standard-deviation 1\n",
    "    OUPUT:\n",
    "        xTr - nxd training data\n",
    "        xTe - mxd testing data\n",
    "    OUPUT:\n",
    "        xTr - pre-processed training data\n",
    "        xTe - pre-processed testing data\n",
    "        s,m - standard deviation and mean of xTr\n",
    "            - any other data should be pre-processed by x-> (x-m)/s\n",
    "    (The size of xTr and xTe should remain unchanged)\n",
    "    \"\"\"\n",
    "    \n",
    "    ntr, d = xTr.shape\n",
    "    nte, _ = xTe.shape\n",
    "    \n",
    "    ## TODO 1\n",
    "    raise NotImplementedError(\"Your code goes here!\")\n",
    "    ## TODO 1\n",
    "\n",
    "    return xTr, xTe, s, m\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> \n",
    "  Now you need to implement three transition functions: <code>ReLU, sigmoid</code> and <code>tanh</code>.\n",
    " </p>\n",
    " <ol>\n",
    "     <li>ReLU: $\\sigma(z) = \\max(0, z)$</li>\n",
    "     <li>Sigmoid: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$</li>\n",
    "     <li>tanh: $\\sigma(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$</li>\n",
    " </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def get_transition_func(transtype):\n",
    "    \"\"\"\n",
    "    Given the type, gets a specific transition function\n",
    "    INPUT:\n",
    "        transtype - \"sigmoid\", \"tanh\", \"ReLU\", \"sin\"\n",
    "    OUTPUT:\n",
    "        trans_func - transition function (function)\n",
    "        trans_func_der - derivative of the transition function (function)\n",
    "\n",
    "    (type must be one of the defined transition functions)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert transtype in [\"sigmoid\", \"tanh\", \"ReLU\",\"sin\"]\n",
    "    if transtype == \"sin\":   \n",
    "        trans_func = lambda z: np.sin(z)\n",
    "        trans_func_der = lambda z: np.cos(z)\n",
    "    ## TODO 2\n",
    "    raise NotImplementedError(\"Your code goes here!\")\n",
    "    ## TODO 2\n",
    "    \n",
    "    return trans_func, trans_func_der\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If everything is correct we can now plot these functions and check their gradients. The gradient errors should all be very small (less than $10^{-10}$.) </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numericalgradient(fun,x,e):\n",
    "    dh = 0\n",
    "    nx = x    # copy the weight vector\n",
    "    nx += e  # perturn dimension i\n",
    "    l1 = fun(nx) # compute loss\n",
    "    nx -= 2*e # perturm dimension i again\n",
    "    l2 = fun(nx) # compute loss\n",
    "    dh = (l1 - l2)/(2*e) # the gradient is the slope of the loss\n",
    "    return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(1, 4, sharex=True, sharey=True)\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(3)\n",
    "names = [\"sigmoid\",\"tanh\",\"ReLU\",\"sin\"]\n",
    "\n",
    "for idx, name in enumerate(names):\n",
    "    # plot stuff\n",
    "    f, delta_f = get_transition_func(name)\n",
    "    x = np.arange(-5, 5, 0.1)\n",
    "    axarr[idx].plot(x, f(x))\n",
    "    axarr[idx].axis([-5,5,-1,1])\n",
    "    axarr[idx].title.set_text(name)\n",
    "    axarr[idx].grid(True)\n",
    "    \n",
    "    # check gradients\n",
    "    print(\"%s gradient check at x=1: \" % name, end='')\n",
    "    dh = numericalgradient(f,1,1e-5)\n",
    "    dy = delta_f(1)\n",
    "    num = np.linalg.norm(dh-dy)\n",
    "    denom = np.linalg.norm(dh+dy)\n",
    "    graderror = num/denom if denom != 0 else 0\n",
    "    if graderror < 1e-10:\n",
    "        print(\"passed \", end='')\n",
    "    else:\n",
    "        print(\"FAILED \", end='')\n",
    "    print('at x=-1: ', end='')\n",
    "    dh2 = numericalgradient(f,-1,1e-5)\n",
    "    dy2 = delta_f(-1)\n",
    "    num = np.linalg.norm(dh2-dy2)\n",
    "    denom = np.linalg.norm(dh2+dy2)\n",
    "    graderror += num/denom if denom != 0 else 0\n",
    "    if graderror < 1e-10:\n",
    "        print(\"passed\")\n",
    "    else:\n",
    "        print(\"FAILED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The following function will randomly generate initial weights for the neural network.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initweights(specs):\n",
    "    \"\"\"\n",
    "    Given a specification of the neural network, output a random weight array\n",
    "    INPUT:\n",
    "        specs - array of length m+1\n",
    "    \n",
    "    OUTPUT:\n",
    "        weights - array of length m, each element is a matrix\n",
    "            where size(weights[i]) = (specs[i], specs[i+1])\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    for i in range(len(specs) - 1):\n",
    "        weights.append(np.random.randn(specs[i], specs[i+1]))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  <li><p> Now implement the forward pass function\n",
    "  <pre>forward_pass(W,xTr,trans_func)</pre>\n",
    "  It takes the weights for the network, the training data, and the transition function to be used between layers.  It should output the result at each node for the forward pass. Each layer has two outputs, $A[i]$ and $Z[i]$, where $A[i]=Z[i-1] * weights[i-1]$ and $Z[i]=trans\\_func(A[i])$. $weights[0]$ stores the weights for the first layer of the network.\n",
    "  </p>\n",
    "  </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def forward_pass(weights, xTr, trans_func):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        weights - weights (cell array of length m)\n",
    "        xTr - nxd matrix (each row is an input vector)\n",
    "        trans_func - transition function to apply for inner layers\n",
    "    \n",
    "    OUTPUTS:\n",
    "        A, Z - result of forward pass (cell array of length m+1)\n",
    "    \n",
    "    Hint:\n",
    "        Make sure A[0]=Z[0]=xTr and A[m] = Z[m] (Why?)\n",
    "    \"\"\"\n",
    "    \n",
    "    ## TODO 3\n",
    "    raise NotImplementedError(\"Your code goes here!\")\n",
    "    ## TODO 3\n",
    "    return A, Z\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights=initweights([2,20,20,1])\n",
    "f,delta_f=get_transition_func(\"sigmoid\")\n",
    "x=np.random.rand(100,2);\n",
    "A,Z=forward_pass(weights,x,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <li><p> Now compute the loss for the network\n",
    "  <pre>compute_loss(zs, yTr)</pre>\n",
    "  It takes the output of the forward pass and the training labels.  It should compute the loss for the entire training set averaging over all the points:\n",
    "  $$L(x, y) = \\frac{1}{2n}(H(x) - y)^2$$\n",
    "  </p>\n",
    "  </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def compute_loss(Z, yTr):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        Z - output of forward pass (cell array of length m+1)\n",
    "        yTr - array of length n\n",
    "    \n",
    "    OUTPUTS:\n",
    "        loss - the average squared loss obtained with Z and yTr (scalar)\n",
    "    \"\"\"\n",
    "    \n",
    "    delta = Z[-1].flatten() - yTr.flatten()\n",
    "    n = len(yTr)\n",
    "    loss = 0\n",
    "\n",
    "    ## TODO 4\n",
    "    raise NotImplementedError(\"Your code goes here!\")\n",
    "    ## TODO 4\n",
    "\n",
    "    return loss\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yTr = np.random.rand(100)\n",
    "compute_loss(Z,yTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <li><p> Now implement <i>back propagation</i> \n",
    "  <pre>backprop(W, as, zs, yTr,  der_trans_func)</pre>\n",
    "  to compute the gradient for the weights and bias terms. \n",
    "  It takes the weights for the network, the outputs of the forward pass, the training labels, and the derivative of the transition function.  Use the chain rule to calculate the gradient of the weights.\n",
    "  </p></li>\n",
    "  \t\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def backprop(weights, A, Z, yTr, delta_f):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        weights - weights (cell array of length m)\n",
    "        A - output of forward pass (cell array of length m+1)\n",
    "        Z - output of forward pass (cell array of length m+1)\n",
    "        yTr - array of length n\n",
    "        delta_f - derivative of transition function to apply for inner layers\n",
    "    \n",
    "    OUTPUTS:\n",
    "        gradient - the gradient at w (cell array of length m)\n",
    "    \"\"\"\n",
    "    \n",
    "    yTr = yTr.reshape(-1,1)\n",
    "    n,_ = yTr.shape\n",
    "    delta = (Z[-1].flatten() - yTr.flatten()).reshape(-1, 1)\n",
    "    \n",
    "    # compute gradient with back-prop\n",
    "    gradients = []\n",
    "    \n",
    "    ## TODO 5\n",
    "    raise NotImplementedError(\"Your code goes here!\")\n",
    "    ## TODO 5\n",
    "    \n",
    "    return gradients\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The following function will plot the results of prediction and the loss.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(x, y, Z, losses):\n",
    "    fig, axarr = plt.subplots(1, 2)\n",
    "    fig.set_figwidth(12)\n",
    "    fig.set_figheight(4)\n",
    "\n",
    "    axarr[0].plot(x, y)\n",
    "    axarr[0].plot(x, Z[-1].flatten())\n",
    "    axarr[0].set_ylabel('$f(x)$')\n",
    "    axarr[0].set_xlabel('$x$')\n",
    "    axarr[0].legend(['Actual', 'Predicted'])\n",
    "\n",
    "    axarr[1].semilogy(losses)\n",
    "    axarr[1].title.set_text('Loss')\n",
    "    axarr[1].set_xlabel('Epoch')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    We can use the gradient from back propagation to update the weights in each layer. There are quite a few ways to do the update. Here we demonstrate three variations of gradient descent.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 5, 0.1)\n",
    "y = (x ** 2 + 10*np.sin(x))\n",
    "\n",
    "x2d = np.concatenate([x, np.ones(x.shape)]).reshape(2, -1).T\n",
    "\n",
    "weights = initweights([2,200,1])\n",
    "momentum = np.copy(weights)\n",
    "for j in range(len(weights)):\n",
    "    momentum[j] = momentum[j] * 0\n",
    "    \n",
    "alpha = 0.01\n",
    "M = 10000\n",
    "beta = 0.8\n",
    "\n",
    "losses = np.zeros(M)\n",
    "t0 = time.time()\n",
    "for i in range(M):\n",
    "    f, delta_f = get_transition_func(\"ReLU\")\n",
    "    A, Z = forward_pass(weights, x2d, f)\n",
    "    losses[i] = compute_loss(Z,y)\n",
    "    gradients = backprop(weights,A,Z,y,delta_f)\n",
    "    for j in range(len(weights)):\n",
    "        momentum[j] = beta * momentum[j] + (1 - beta) * gradients[j]\n",
    "        weights[j] -= alpha * momentum[j]\n",
    "t1 = time.time()\n",
    "print('Elapsed time: %.2fs (probably faster than Julia)' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_results(x, y, Z, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "  If you did everything correctly, the result should look similar to this image:\n",
    "</p>\n",
    "<center>\n",
    "   <img src=\"./p8/plot.png\" width=\"800px\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom step-size decrease schedule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "x = np.arange(0, 5, 0.1)\n",
    "y = (x ** 2 + 10*np.sin(x))\n",
    "x2d = np.concatenate([x, np.ones(x.shape)]).reshape(2, -1).T\n",
    "\n",
    "# transition function\n",
    "f, delta_f = get_transition_func(\"ReLU\")\n",
    "\n",
    "# initialize weights, historical gradients, losses\n",
    "weights = initweights([2,200,1])\n",
    "losses = np.zeros(M)\n",
    "\n",
    "hist_grad = np.copy(weights)\n",
    "for j in range(len(weights)):\n",
    "    hist_grad[j] = hist_grad[j] * 0\n",
    "\n",
    "alpha = 0.02\n",
    "M = 10000\n",
    "beta = 0.8\n",
    "eps = 1e-6\n",
    "\n",
    "losses = np.zeros(M)\n",
    "t0 = time.time()\n",
    "for i in range(M):\n",
    "    f, delta_f = get_transition_func(\"ReLU\")\n",
    "    A, Z = forward_pass(weights, x2d, f)\n",
    "    losses[i] = compute_loss(Z,y)\n",
    "    gradients = backprop(weights,A,Z,y,delta_f)\n",
    "    for j in range(len(weights)):\n",
    "        if i != 0:\n",
    "            hist_grad[j] = beta * hist_grad[j] + (1 - beta) * gradients[j] ** 2\n",
    "        else:\n",
    "            hist_grad[j] = gradients[j] ** 2\n",
    "        adj_grad = gradients[j] / (eps + np.sqrt(hist_grad[j]))\n",
    "        weights[j] -= alpha * adj_grad\n",
    "t1 = time.time()\n",
    "print('Elapsed time: %.2fs (probably faster than Julia)' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_results(x, y, Z, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "x = np.arange(0, 5, 0.1)\n",
    "y = (x ** 2 + 10*np.sin(x))\n",
    "x2d = np.concatenate([x, np.ones(x.shape)]).reshape(2, -1).T\n",
    "\n",
    "# transition function\n",
    "f, delta_f = get_transition_func(\"ReLU\")\n",
    "\n",
    "# initialize weights, historical gradients, losses\n",
    "weights = initweights([2,200,1])\n",
    "losses = np.zeros(M)\n",
    "\n",
    "hist_grad = np.copy(weights)\n",
    "for j in range(len(weights)):\n",
    "    hist_grad[j] = hist_grad[j] * 0\n",
    "\n",
    "alpha = 0.02\n",
    "M = 10000\n",
    "beta = 0.8\n",
    "eps = 1e-6\n",
    "\n",
    "losses = np.zeros(M)\n",
    "t0 = time.time()\n",
    "for i in range(M):\n",
    "    f, delta_f = get_transition_func(\"ReLU\")\n",
    "    A, Z = forward_pass(weights, x2d, f)\n",
    "    losses[i] = compute_loss(Z,y)\n",
    "    gradients = backprop(weights,A,Z,y,delta_f)\n",
    "    for j in range(len(weights)):\n",
    "        hist_grad[j] += gradients[j] ** 2\n",
    "        adj_grad = gradients[j] / (eps + np.sqrt(hist_grad[j]))\n",
    "        weights[j] -= alpha * adj_grad\n",
    "t1 = time.time()\n",
    "print('Elapsed time: %.2fs (probably faster than Julia)' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_results(x, y, Z, losses)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
